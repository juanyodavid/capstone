{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib as pl\n",
    "import sklearn\n",
    "import pathlib as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionarypath = '../dataset/dictionary/Loughran-McDonald_MasterDictionary_1993-2021.csv'\n",
    "wds_dict = pd.read_csv(dictionarypath)\n",
    "positive_wds = [row['Word'].lower() for i, row in wds_dict.iterrows() if row['Positive']!=0]\n",
    "negative_wds = [row['Word'].lower() for i, row in wds_dict.iterrows() if row['Negative']!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../dataset/finRep/FinancialPhraseBank/all-data.csv'\n",
    "df = pd.read_csv(path, encoding = \"ISO-8859-1\", names = ['sentiment', 'sentence'])\n",
    "# print(df[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to install keybert\n",
    "# !pip install nltk\n",
    "# !pip install keybert\n",
    "# !pip install sentence-transformers\n",
    "# !pip install git+https://github.com/LIAAD/yake\n",
    "### To show the execution times\n",
    "# !pip install ipython-autotime\n",
    "# %load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net interest income totaled EUR 15.9 mn , compared to EUR 15.6 mn a year earlier .\n",
      "the sentiment is : negative\n"
     ]
    }
   ],
   "source": [
    "# choose a random sample\n",
    "text = df.sample(1)['sentence'].iloc[0]\n",
    "label = df.sample(1)['sentiment'].iloc[0]\n",
    "# text = \"you are not good at trading \"\n",
    "print(text)\n",
    "print(\"the sentiment is : %s\"%label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_keyword_list(keyword_list):\n",
    "    keys = []\n",
    "    for keyword in keyword_list:\n",
    "        keys.append(keyword[0])\n",
    "    return keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['net interest income totaled eur 15', 'eur 15', 'year earlier', '9 mn', '6 mn', 'compared']\n"
     ]
    }
   ],
   "source": [
    "from rake_nltk import Rake\n",
    "rake_nltk_var = Rake()\n",
    "\n",
    "rake_nltk_var.extract_keywords_from_text(text)\n",
    "rake_keywords = rake_nltk_var.get_ranked_phrases()\n",
    "print(rake_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import them\n",
    "from keybert import KeyBERT\n",
    "from sentence_transformers import SentenceTransformer\n",
    "sent_trans = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "keyBERT_model = KeyBERT(model = sent_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_terms(document, n_gram_range = (3,3), \n",
    "                  top_N=5, model=keyBERT_model, diversity_threshold = 0.7):\n",
    "\n",
    "  keywords = model.extract_keywords(document, stop_words='english', \n",
    "                                    keyphrase_ngram_range=(1, 3),\n",
    "                                    use_mmr=True, \n",
    "                                    diversity = diversity_threshold,\n",
    "                                    top_n = top_N)\n",
    "  \n",
    "  return sorted(keywords, key=lambda tup:(-tup[1], tup[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['income totaled eur', 'mn year earlier', '15', 'net', 'compared']\n"
     ]
    }
   ],
   "source": [
    "# print(f'Text: {text}.\\nLength: {len(text.split())}')\n",
    "keybert_keywords = extract_terms(text)\n",
    "keybert_keywords = normalize_keyword_list(keybert_keywords)\n",
    "\n",
    "print(keybert_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['income totaled EUR', 'Net interest income', 'interest income totaled', 'totaled EUR', 'Net interest', 'year earlier', 'compared to EUR', 'interest income', 'income totaled', 'EUR', 'Net', 'compared', 'earlier', 'interest', 'income', 'totaled', 'year']\n"
     ]
    }
   ],
   "source": [
    "import yake\n",
    "kw_extractor = yake.KeywordExtractor()\n",
    "yake_keywords = kw_extractor.extract_keywords(text)\n",
    "yake_keywords = normalize_keyword_list(yake_keywords)\n",
    "\n",
    "print(yake_keywords)\n",
    "\n",
    "\n",
    "# print(\"SECOND FORM:\")\n",
    "# language = \"en\"\n",
    "# max_ngram_size = 3\n",
    "# deduplication_threshold = 0.9\n",
    "# deduplication_algo = 'seqm'\n",
    "# windowSize = 1\n",
    "# numOfKeywords = 20\n",
    "\n",
    "# custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, dedupFunc=deduplication_algo, windowsSize=windowSize, top=numOfKeywords, features=None)\n",
    "# keywords = custom_kw_extractor.extract_keywords(text)\n",
    "\n",
    "# for kw in keywords:\n",
    "#     print(kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rake: ['net interest income totaled eur 15', 'eur 15', 'year earlier', '9 mn', '6 mn', 'compared']\n",
      "Keybert: ['income totaled eur', 'mn year earlier', '15', 'net', 'compared']\n",
      "Yake: ['income totaled EUR', 'Net interest income', 'interest income totaled', 'totaled EUR', 'Net interest', 'year earlier', 'compared to EUR', 'interest income', 'income totaled', 'EUR', 'Net', 'compared', 'earlier', 'interest', 'income', 'totaled', 'year']\n"
     ]
    }
   ],
   "source": [
    "Keywords = {}\n",
    "Keywords[\"rake\"] = rake_keywords\n",
    "Keywords[\"keybert\"] = keybert_keywords\n",
    "Keywords[\"yake\"] = yake_keywords\n",
    "print(\"Rake: %s\" % Keywords[\"rake\"]) \n",
    "print(\"Keybert: %s\" % Keywords[\"keybert\"]) \n",
    "print(\"Yake: %s\" % Keywords[\"yake\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK-VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['income totaled eur', 'mn year earlier', '15', 'net', 'compared']\n"
     ]
    }
   ],
   "source": [
    "print(keybert_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('income', 0), ('totaled', 0), ('eur', 0)], [('mn', 0), ('year', 0), ('earlier', 0)], [('15', 0)], [('net', 0)], [('compared', 0)]]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "result = []\n",
    "for sentence in keybert_keywords:\n",
    "    s = sentence.split()\n",
    "    keyword_sen = []\n",
    "    for word in s:\n",
    "        if (sid.polarity_scores(word)['compound']) >= 0.5:\n",
    "            keyword_sen.append(tuple((word, 1)))\n",
    "        elif (sid.polarity_scores(word)['compound']) <= -0.5:\n",
    "            keyword_sen.append(tuple((word, -1)))\n",
    "        else:\n",
    "            keyword_sen.append(tuple((word, 0)))\n",
    "    result.append(keyword_sen)\n",
    "\n",
    "print(result)              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('income', 0), ('totaled', 0), ('eur', 0)], [('mn', 0), ('year', 0), ('earlier', 0)], [('15', 0)], [('net', 0)], [('compared', 0)]]\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "result = []\n",
    "for sentence in keybert_keywords:\n",
    "    s = sentence.split()\n",
    "    keyword_sen = []\n",
    "    for word in s:\n",
    "        testimonial = TextBlob(word)\n",
    "        if testimonial.sentiment.polarity >= 0.5:\n",
    "            keyword_sen.append(tuple((word, 1)))\n",
    "        elif testimonial.sentiment.polarity <= -0.5:\n",
    "            keyword_sen.append(tuple((word, -1)))\n",
    "        else:\n",
    "            keyword_sen.append(tuple((word, 0)))\n",
    "    result.append(keyword_sen)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_words(keyword_list):\n",
    "    result = []\n",
    "    for sentence in keyword_list:\n",
    "        words = sentence.split()\n",
    "        keyword_sen = []\n",
    "        for word in words:\n",
    "            if word in positive_wds:\n",
    "                keyword_sen.append(tuple((word, 1)))\n",
    "            elif word in negative_wds:\n",
    "                keyword_sen.append(tuple((word, -1)))\n",
    "            else:\n",
    "                keyword_sen.append(tuple((word, 0)))\n",
    "        result.append(keyword_sen)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('income', 0), ('totaled', 0), ('eur', 0)], [('mn', 0), ('year', 0), ('earlier', 0)], [('15', 0)], [('net', 0)], [('compared', 0)]]\n"
     ]
    }
   ],
   "source": [
    "keybert_sen = compare_words(keybert_keywords) #keybert\n",
    "\n",
    "print(keybert_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# !python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:12: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  if sys.path[0] == \"\":\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:14: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['income totaled eur', 'net interest income totaled eur 15', 'year earlier', 'compared', 'mn year earlier', '15']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "FinalKeys = []\n",
    "for keybert in Keywords[\"keybert\"]:\n",
    "    word1 = nlp(keybert)\n",
    "    for rake in Keywords[\"rake\"]:\n",
    "        word2 = nlp(rake)\n",
    "        for yake in Keywords[\"yake\"]:\n",
    "            word3 = nlp(yake)\n",
    "            if word3.similarity(word1) > 0.7 and (not(keybert in FinalKeys)):\n",
    "                FinalKeys.append(keybert)\n",
    "            if word3.similarity(word2) > 0.7 and not(rake in FinalKeys):\n",
    "                FinalKeys.append(rake)\n",
    "            if word2.similarity(word1) > 0.7 and not(keybert in FinalKeys):\n",
    "                FinalKeys.append(keybert)\n",
    "print(FinalKeys)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
