{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib as pl\n",
    "import sklearn\n",
    "import pathlib as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionarypath = '../dataset/dictionary/Loughran-McDonald_MasterDictionary_1993-2021.csv'\n",
    "wds_dict = pd.read_csv(dictionarypath)\n",
    "positive_wds = [row['Word'].lower() for i, row in wds_dict.iterrows() if row['Positive']!=0]\n",
    "negative_wds = [row['Word'].lower() for i, row in wds_dict.iterrows() if row['Negative']!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>links</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>title</td>\n",
       "      <td>body</td>\n",
       "      <td>links</td>\n",
       "      <td>source</td>\n",
       "      <td>target</td>\n",
       "      <td>sentiment_score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dairy Crest loses a third of Morrisons milk co...</td>\n",
       "      <td>(Reuters) - Dairy Crest Group Plc DCG.L lost a...</td>\n",
       "      <td>https://www.reuters.com/article/uk-dairy-crest...</td>\n",
       "      <td>www.reuters.com</td>\n",
       "      <td>Morrisons</td>\n",
       "      <td>-0.161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pearson expects to return to growth this year</td>\n",
       "      <td>LONDON (Reuters) - Pearson PSON.L said it expe...</td>\n",
       "      <td>https://www.reuters.com/article/uk-pearson-res...</td>\n",
       "      <td>www.reuters.com</td>\n",
       "      <td>Pearson</td>\n",
       "      <td>0.446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                                              title   \n",
       "1  Dairy Crest loses a third of Morrisons milk co...   \n",
       "2      Pearson expects to return to growth this year   \n",
       "\n",
       "                                                body  \\\n",
       "0                                               body   \n",
       "1  (Reuters) - Dairy Crest Group Plc DCG.L lost a...   \n",
       "2  LONDON (Reuters) - Pearson PSON.L said it expe...   \n",
       "\n",
       "                                               links           source  \\\n",
       "0                                              links           source   \n",
       "1  https://www.reuters.com/article/uk-dairy-crest...  www.reuters.com   \n",
       "2  https://www.reuters.com/article/uk-pearson-res...  www.reuters.com   \n",
       "\n",
       "      target  sentiment_score  \n",
       "0     target  sentiment_score  \n",
       "1  Morrisons           -0.161  \n",
       "2    Pearson            0.446  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path = '../dataset/finRep/FinancialPhraseBank/all-data.csv'\n",
    "# df = pd.read_csv(path, encoding = \"ISO-8859-1\", names = ['sentiment', 'sentence'])\n",
    "# # print(df[:10])\n",
    "path = '../dataset/finRep/FiQA/fiqa_extended.csv'\n",
    "df = pd.read_csv(path, encoding = \"ISO-8859-1\", names = ['title', 'body','links','source','target','sentiment_score'])\n",
    "df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to install keybert\n",
    "# !pip install nltk\n",
    "# !pip install keybert\n",
    "# !pip install sentence-transformers\n",
    "# !pip install git+https://github.com/LIAAD/yake\n",
    "### To show the execution times\n",
    "# !pip install ipython-autotime\n",
    "# %load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WASHINGTON Reuters AstraZeneca Plcs diabetes drug Onglyza and Takeda Pharmaceutical Co.s rival product Nesina should carry information about the risk of heart failure, an advisory committee to the U.S. Food and Drug Administration said on Tuesday.Heart failure is a condition in which the heart cannot pump enough blood to meet the bodys needs. It can be caused by coronary artery disease, high blood pressure, diabetes or obesity, according to the Mayo Clinic.The panel reviewed safety data on AstraZenecas Onglyza and Takedas Nesina. The drugs belong to a class known as DPP4 inhibitors which also includes Merck Co.s Januvia. Januvia data from a trial known as TECOS will be presented at a scientific meeting in June.The studies were requested by the FDA as part of a broader investigation into the safety of diabetes drugs. In December 2008, the agency issued guidance requiring companies to conduct studies to show the drugs did not increase cardiovascular risk.The panel found that neither Onglyza nor Nesina increased the risk of cardiovascular death, stroke or heart attack. But data showed a statistically significant increase in the risk of heart failure with Onglyza and an increased risk with Nesina that did not reach statistical significance.Some panelists said that even though the heart failure risk seen with Nesina was modest, they suspect the risk will turn out to be a DPP4 class effect and therefore worth including.The panel did not recommend any restrictions on prescribing the drugs. AstraZenecas U.S. shares rose 3.2 percent. Takedas shares closed down 0.67 percent before the vote was announced. Mercks shares rose 1.8 percent.Given the absence of a serious safety issue for either Onglyza or Takedas Nesina suggests that, barring an overtly negative signal in TECOS, growth of the DPP4 class is likely to continue without a meaningful change in current prescribing trends, Leerink analyst Seamus Fernandez said in a research note.A preliminary FDA review of Onglyza published on Friday found an increase in the rate of death from all causes. Panel members said they were moderately concerned about the signal but said their concern was muted by the fact that the causes of death were varied and lacked any common theme.Onglyza, known also as saxagliptin, was approved in 2009. Nesina, or alogliptin, was approved in 2013. The FDA is not obliged to follow the advice of its advisory panels but typically does so.\n",
      "the sentiment is : 0.334\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = df.sample(1).iloc[0]['body']\n",
    "text = re.sub(r'[^A-Za-z0-9 .,;]+', '', text)\n",
    "text = re.sub(' +', ' ', text)\n",
    "# df =df.drop(index = 0)\n",
    "    # df[:3]\n",
    "print(text)\n",
    "label = df.sample(1)['sentiment_score'].iloc[0]\n",
    "\n",
    "print(\"the sentiment is : %s\"%label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_keyword_list(keyword_list):\n",
    "    keys = []\n",
    "    for keyword in keyword_list:\n",
    "        keys.append(keyword[0])\n",
    "    return keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['washington reuters astrazeneca plcs diabetes drug onglyza', 'agency issued guidance requiring companies', 'leerink analyst seamus fernandez said', 'heart cannot pump enough blood', 'mercks shares rose 1', 'also includes merck co', 'panel reviewed safety data', 'heart failure risk seen', 'drug administration said', 'high blood pressure', 'shares rose 3', 'takeda pharmaceutical co', 'therefore worth including', 'takedas shares closed', 'reach statistical significance', 'coronary artery disease', 'serious safety issue', 'statistically significant increase', 'panel members said', 'overtly negative signal', 'current prescribing trends', 'rival product nesina', 'preliminary fda review', 'dpp4 class effect', 'takedas nesina suggests', 'increase cardiovascular risk', 'heart failure', 'heart failure', 'heart failure', 'known also', 'heart attack', 'diabetes drugs', 'panelists said', 'dpp4 class', 'data showed', 'takedas nesina', 'panel found', 'onglyza published', 'neither onglyza', 'either onglyza', 'astrazenecas onglyza', 'dpp4 inhibitors', 'class known', 'januvia data', 'trial known', 'scientific meeting', 'research note', 'moderately concerned', 'meaningful change', 'mayo clinic', 'friday found', 'even though', 'december 2008', 'continue without', 'common theme', 'carry information', 'broader investigation', 'bodys needs', 'advisory panels', 'advisory committee', '8 percent', '67 percent', '2 percent', 'nesina increased', 'increased risk', 'cardiovascular death', 'drugs belong', 'conduct studies', 'astrazenecas u', 'diabetes', 'said', 'safety', 'panel', 'onglyza', 'onglyza', 'increase', 'signal', 'prescribing', 'risk', 'risk', 'risk', 'risk', 'nesina', 'nesina', 'nesina', 'fda', 'fda', 'u', 'studies', 'januvia', 'drugs', 'drugs', 'death', 'death', 'vote', 'varied', 'typically', 'turn', 'tuesday', 'tecos', 'tecos', 'suspect', 'stroke', 'show', 'saxagliptin', 'restrictions', 'requested', 'recommend', 'rate', 'presented', 'part', 'obliged', 'obesity', 'muted', 'modest', 'meet', 'likely', 'lacked', 'june', 'growth', 'given', 'food', 'follow', 'fact', 'condition', 'concern', 'causes', 'causes', 'caused', 'barring', 'approved', 'approved', 'announced', 'alogliptin', 'advice', 'according', 'absence', '2013', '2009', '0']\n"
     ]
    }
   ],
   "source": [
    "from rake_nltk import Rake\n",
    "rake_nltk_var = Rake()\n",
    "\n",
    "rake_nltk_var.extract_keywords_from_text(text)\n",
    "rake_keywords = rake_nltk_var.get_ranked_phrases()\n",
    "print(rake_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import them\n",
    "from keybert import KeyBERT\n",
    "from sentence_transformers import SentenceTransformer\n",
    "sent_trans = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "keyBERT_model = KeyBERT(model = sent_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_terms(document, n_gram_range = (3,3), \n",
    "                  top_N=5, model=keyBERT_model, diversity_threshold = 0.7):\n",
    "\n",
    "  keywords = model.extract_keywords(document, stop_words='english', \n",
    "                                    keyphrase_ngram_range=(1, 3),\n",
    "                                    use_mmr=True, \n",
    "                                    diversity = diversity_threshold,\n",
    "                                    top_n = top_N)\n",
    "  \n",
    "  return sorted(keywords, key=lambda tup:(-tup[1], tup[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['diabetes drug onglyza', 'heart failure risk', 'takedas shares closed', 'dpp4 class effect', 'panels']\n"
     ]
    }
   ],
   "source": [
    "# print(f'Text: {text}.\\nLength: {len(text.split())}')\n",
    "keybert_keywords = extract_terms(text)\n",
    "keybert_keywords = normalize_keyword_list(keybert_keywords)\n",
    "\n",
    "print(keybert_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['WASHINGTON Reuters AstraZeneca', 'Reuters AstraZeneca Plcs', 'WASHINGTON Reuters', 'Plcs diabetes drug', 'AstraZeneca Plcs diabetes', 'rival product Nesina', 'Pharmaceutical Co.s rival', 'diabetes drug Onglyza', 'Takeda Pharmaceutical', 'Plcs diabetes', 'Drug Administration', 'Reuters AstraZeneca', 'AstraZeneca Plcs', 'Takedas Nesina', 'Nesina', 'Onglyza', 'rival product', 'carry information', 'meet the bodys', 'product Nesina']\n"
     ]
    }
   ],
   "source": [
    "import yake\n",
    "kw_extractor = yake.KeywordExtractor()\n",
    "yake_keywords = kw_extractor.extract_keywords(text)\n",
    "yake_keywords = normalize_keyword_list(yake_keywords)\n",
    "\n",
    "print(yake_keywords)\n",
    "\n",
    "\n",
    "# print(\"SECOND FORM:\")\n",
    "# language = \"en\"\n",
    "# max_ngram_size = 3\n",
    "# deduplication_threshold = 0.9\n",
    "# deduplication_algo = 'seqm'\n",
    "# windowSize = 1\n",
    "# numOfKeywords = 20\n",
    "\n",
    "# custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, dedupFunc=deduplication_algo, windowsSize=windowSize, top=numOfKeywords, features=None)\n",
    "# keywords = custom_kw_extractor.extract_keywords(text)\n",
    "\n",
    "# for kw in keywords:\n",
    "#     print(kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rake: ['washington reuters astrazeneca plcs diabetes drug onglyza', 'agency issued guidance requiring companies', 'leerink analyst seamus fernandez said', 'heart cannot pump enough blood', 'mercks shares rose 1', 'also includes merck co', 'panel reviewed safety data', 'heart failure risk seen', 'drug administration said', 'high blood pressure', 'shares rose 3', 'takeda pharmaceutical co', 'therefore worth including', 'takedas shares closed', 'reach statistical significance', 'coronary artery disease', 'serious safety issue', 'statistically significant increase', 'panel members said', 'overtly negative signal', 'current prescribing trends', 'rival product nesina', 'preliminary fda review', 'dpp4 class effect', 'takedas nesina suggests', 'increase cardiovascular risk', 'heart failure', 'heart failure', 'heart failure', 'known also', 'heart attack', 'diabetes drugs', 'panelists said', 'dpp4 class', 'data showed', 'takedas nesina', 'panel found', 'onglyza published', 'neither onglyza', 'either onglyza', 'astrazenecas onglyza', 'dpp4 inhibitors', 'class known', 'januvia data', 'trial known', 'scientific meeting', 'research note', 'moderately concerned', 'meaningful change', 'mayo clinic', 'friday found', 'even though', 'december 2008', 'continue without', 'common theme', 'carry information', 'broader investigation', 'bodys needs', 'advisory panels', 'advisory committee', '8 percent', '67 percent', '2 percent', 'nesina increased', 'increased risk', 'cardiovascular death', 'drugs belong', 'conduct studies', 'astrazenecas u', 'diabetes', 'said', 'safety', 'panel', 'onglyza', 'onglyza', 'increase', 'signal', 'prescribing', 'risk', 'risk', 'risk', 'risk', 'nesina', 'nesina', 'nesina', 'fda', 'fda', 'u', 'studies', 'januvia', 'drugs', 'drugs', 'death', 'death', 'vote', 'varied', 'typically', 'turn', 'tuesday', 'tecos', 'tecos', 'suspect', 'stroke', 'show', 'saxagliptin', 'restrictions', 'requested', 'recommend', 'rate', 'presented', 'part', 'obliged', 'obesity', 'muted', 'modest', 'meet', 'likely', 'lacked', 'june', 'growth', 'given', 'food', 'follow', 'fact', 'condition', 'concern', 'causes', 'causes', 'caused', 'barring', 'approved', 'approved', 'announced', 'alogliptin', 'advice', 'according', 'absence', '2013', '2009', '0']\n",
      "Keybert: ['diabetes drug onglyza', 'heart failure risk', 'takedas shares closed', 'dpp4 class effect', 'panels']\n",
      "Yake: ['WASHINGTON Reuters AstraZeneca', 'Reuters AstraZeneca Plcs', 'WASHINGTON Reuters', 'Plcs diabetes drug', 'AstraZeneca Plcs diabetes', 'rival product Nesina', 'Pharmaceutical Co.s rival', 'diabetes drug Onglyza', 'Takeda Pharmaceutical', 'Plcs diabetes', 'Drug Administration', 'Reuters AstraZeneca', 'AstraZeneca Plcs', 'Takedas Nesina', 'Nesina', 'Onglyza', 'rival product', 'carry information', 'meet the bodys', 'product Nesina']\n"
     ]
    }
   ],
   "source": [
    "Keywords = {}\n",
    "Keywords[\"rake\"] = rake_keywords\n",
    "Keywords[\"keybert\"] = keybert_keywords\n",
    "Keywords[\"yake\"] = yake_keywords\n",
    "print(\"Rake: %s\" % Keywords[\"rake\"]) \n",
    "print(\"Keybert: %s\" % Keywords[\"keybert\"]) \n",
    "print(\"Yake: %s\" % Keywords[\"yake\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK-VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stonesoft', 'simplify network security', 'global provider proven', 'management', 'generation solutions simplify']\n"
     ]
    }
   ],
   "source": [
    "print(keybert_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('stonesoft', 0)], [('simplify', 0), ('network', 0), ('security', 0)], [('global', 0), ('provider', 0), ('proven', 0)], [('management', 0)], [('generation', 0), ('solutions', 0), ('simplify', 0)]]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "result = []\n",
    "for sentence in keybert_keywords:\n",
    "    s = sentence.split()\n",
    "    keyword_sen = []\n",
    "    for word in s:\n",
    "        if (sid.polarity_scores(word)['compound']) >= 0.5:\n",
    "            keyword_sen.append(tuple((word, 1)))\n",
    "        elif (sid.polarity_scores(word)['compound']) <= -0.5:\n",
    "            keyword_sen.append(tuple((word, -1)))\n",
    "        else:\n",
    "            keyword_sen.append(tuple((word, 0)))\n",
    "    result.append(keyword_sen)\n",
    "\n",
    "print(result)              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('stonesoft', 0)], [('simplify', 0), ('network', 0), ('security', 0)], [('global', 0), ('provider', 0), ('proven', 0)], [('management', 0)], [('generation', 0), ('solutions', 0), ('simplify', 0)]]\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "result = []\n",
    "for sentence in keybert_keywords:\n",
    "    s = sentence.split()\n",
    "    keyword_sen = []\n",
    "    for word in s:\n",
    "        testimonial = TextBlob(word)\n",
    "        if testimonial.sentiment.polarity >= 0.5:\n",
    "            keyword_sen.append(tuple((word, 1)))\n",
    "        elif testimonial.sentiment.polarity <= -0.5:\n",
    "            keyword_sen.append(tuple((word, -1)))\n",
    "        else:\n",
    "            keyword_sen.append(tuple((word, 0)))\n",
    "    result.append(keyword_sen)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_words(keyword_list):\n",
    "    result = []\n",
    "    for sentence in keyword_list:\n",
    "        words = sentence.split()\n",
    "        keyword_sen = []\n",
    "        for word in words:\n",
    "            if word in positive_wds:\n",
    "                keyword_sen.append(tuple((word, 1)))\n",
    "            elif word in negative_wds:\n",
    "                keyword_sen.append(tuple((word, -1)))\n",
    "            else:\n",
    "                keyword_sen.append(tuple((word, 0)))\n",
    "        result.append(keyword_sen)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('stonesoft', 0)], [('simplify', 0), ('network', 0), ('security', 0)], [('global', 0), ('provider', 0), ('proven', 0)], [('management', 0)], [('generation', 0), ('solutions', 0), ('simplify', 0)]]\n"
     ]
    }
   ],
   "source": [
    "keybert_sen = compare_words(keybert_keywords) #keybert\n",
    "\n",
    "print(keybert_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# !python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:12: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  if sys.path[0] == \"\":\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:14: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['simplify network security management', 'stonesoft', 'next generation solutions', 'global provider', 'proven', 'simplify network security', 'global provider proven', 'management', 'generation solutions simplify']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "FinalKeys = []\n",
    "for keybert in Keywords[\"keybert\"]:\n",
    "    word1 = nlp(keybert)\n",
    "    for rake in Keywords[\"rake\"]:\n",
    "        word2 = nlp(rake)\n",
    "        for yake in Keywords[\"yake\"]:\n",
    "            word3 = nlp(yake)\n",
    "            if word3.similarity(word1) > 0.7 and (not(keybert in FinalKeys)):\n",
    "                FinalKeys.append(keybert)\n",
    "            if word3.similarity(word2) > 0.7 and not(rake in FinalKeys):\n",
    "                FinalKeys.append(rake)\n",
    "            if word2.similarity(word1) > 0.7 and not(keybert in FinalKeys):\n",
    "                FinalKeys.append(keybert)\n",
    "print(FinalKeys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPROCESSING"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
