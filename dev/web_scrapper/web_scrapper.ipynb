{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib as pl\n",
    "import sklearn\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path():\n",
    "    path = './links_sentfin_old.csv'\n",
    "    df = pd.read_csv(path, encoding = \"ISO-8859-1\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert dict column to sentiment and drop those with more than one sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ast\n",
    "\n",
    "# to_drop = []\n",
    "# for i in range(len(df.index)):\n",
    "#     aux = ast.literal_eval(df.iloc[i]['Decisions'])\n",
    "#     df.Decisions[i]= list(aux.values())[0]\n",
    "\n",
    "#     if len(aux) > 1: # and len(set(list(aux.values()))) > 1\n",
    "#         to_drop.append(i)\n",
    "# len(to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop(to_drop)\n",
    "# df = df.reset_index(drop=True)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrange(df):\n",
    "    sources = []\n",
    "\n",
    "    from urllib.parse import urlparse\n",
    "\n",
    "    for i in range(len(df.index)):\n",
    "        domain = urlparse(df['links'][i]).netloc\n",
    "        sources.append(domain)\n",
    "\n",
    "    df['source'] = pd.Series(sources)\n",
    "    del sources\n",
    "    df.source.value_counts().sort_values()\n",
    "    df = df[df['source'].str.contains(\"economictimes\")]\n",
    "\n",
    "    df = df.reset_index(drop=True)\n",
    "    # print(len(df.index))\n",
    "    return df\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "economictimes.indiatimes.com     3\n",
       "m.economictimes.com             17\n",
       "Name: source, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.source.value_counts().sort_values().plot(kind = 'barh')\n",
    "# pd.options.display.max_rows = 4000\n",
    "df.source.value_counts().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[df['source'].str.contains(\"reuters\")] #filtering reuters\n",
    "\n",
    "df = df[df['source'].str.contains(\"economictimes\")]\n",
    "\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(start,end,df):\n",
    "    # start = 0 \n",
    "    # end = 20 # creo que ya llegue a este\n",
    "\n",
    "    df = df[start:end]\n",
    "    df = df.reset_index(drop=True)\n",
    "    # print(\"start: %s, end: %s\" % (start,end))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install html5lib\n",
    "# !pip3 install bs4\n",
    "# !pip3 install BeautifulSoup\n",
    "# !pip3 install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "def search_link(df):\n",
    "    soup = []   \n",
    "    for link in df['links']:\n",
    "        URL = link\n",
    "\n",
    "        headers = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Safari/537.36 Edge/12.246\"}\n",
    "        # Here the user agent is for Edge browser on windows 10. You can find your browser user agent from the above given link.\n",
    "        r = requests.get(url=URL, headers=headers)\n",
    "        # print(r.content)\n",
    "\n",
    "        soup.append(BeautifulSoup(r.content, 'html5lib')) # If this line causes an error, run 'pip install html5lib' or install html5lib\n",
    "        # print(soup[-1].prettify())\n",
    "        sleep(40)\n",
    "\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def titles(soup):\n",
    "    titles = []\n",
    "\n",
    "\n",
    "    for s in soup:\n",
    "        # title = s.find('h1', attrs = {\"class\": \"Headline-headline-2FXIq Headline-black-OogpV ArticleHeader-headline-NlAqj\"}) #for reuters\n",
    "\n",
    "        title = s.find('h1', attrs = {\"class\": \"artTitle font_faus\"}) #for economictimes\n",
    "        if title is not None:\n",
    "            titles.append(title.text) \n",
    "        else:\n",
    "            titles.append('HOLAXD')\n",
    "\n",
    "    return titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bodies (soup):\n",
    "    bodies = []\n",
    "\n",
    "    for s in soup:\n",
    "        body = \"\"\n",
    "\n",
    "        # table = s.find(\"div\", attrs = {\"class\": \"TwoColumnsLayout-body-86gsE ArticlePage-body-container-10RhS\"}) #for retuers\n",
    "\n",
    "        table = s.find(\"div\", attrs = {\"class\": \"pageContent flt\"}) #for economictimes\n",
    "        \n",
    "        if table is not None:\n",
    "            # for row in table.findAll(\"p\",\n",
    "                                    # attrs = {\"class\":\"Paragraph-paragraph-2Bgue ArticleBody-para-TD_9x\"}):  #for reuters\n",
    "                # body += row.text + \"\\n\"\n",
    "\n",
    "            table = table.find(\"article\", attrs = {\"class\": \"artData clr\"})\n",
    "\n",
    "            if table is None:\n",
    "                table = s.find(\"div\", attrs = {\"class\": \"pageContent flt\"}).find(\"article\", attrs = {\"class\": \"artData clr paywall\"})\n",
    "\n",
    "            if table is not None:\n",
    "                table = table.find(\"div\", attrs = {\"class\": \"artText\"})\n",
    "\n",
    "                if table is not None:\n",
    "                    for div in table.find_all(\"div\", {'class':\"ar_wrp arwd_ld_chk font_mon clr hide\"}): \n",
    "                        div.decompose()\n",
    "\n",
    "                    bodies.append(table.text)\n",
    "                else:\n",
    "                    bodies.append('HOLAXD')\n",
    "\n",
    "            else:\n",
    "                bodies.append('HOLAXD')\n",
    "\n",
    "        else:\n",
    "            bodies.append('HOLAXD')\n",
    "\n",
    "    return bodies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title'] = pd.Series(titles)\n",
    "df['body'] = pd.Series(bodies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# if file does not exist write header \n",
    "def cargar_csv(df): \n",
    "   if not os.path.isfile('sentfin_extended22.csv'):\n",
    "      df.to_csv('sentfin_extended22.csv', header='column_names', index=False)\n",
    "   else: # else it exists so append without writing the header\n",
    "      df.to_csv('sentfin_extended22.csv', mode='a', header=False, index=False)\n",
    "\n",
    "   # df.to_csv(\"file.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "se termino hasta:  380\n",
      "se termino hasta:  400\n",
      "se termino hasta:  420\n",
      "se termino hasta:  440\n",
      "se termino hasta:  460\n",
      "se termino hasta:  480\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib as pl\n",
    "import sklearn\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "start = 360\n",
    "for end in range(380,500,20):\n",
    "    path_is = get_path()\n",
    "    # print(\"path_is = \", path_is)\n",
    "    arreglo = arrange(path_is)\n",
    "    # print(\"arreglo = \", arreglo)\n",
    "    df = count(start, end,arreglo)\n",
    "    # print(\"df = \", df)\n",
    "    sopa = search_link (df)\n",
    "    # print(\"sopa = \", sopa)\n",
    "    titulos = titles(sopa)\n",
    "    # print(\"titulos = \", titulos)\n",
    "    df['title'] = pd.Series(titulos)\n",
    "    df['body'] = pd.Series(bodies(sopa))\n",
    "    # print (\"partido:\",df[start:end])\n",
    "    # print(\"completo:\",df)\n",
    "    cargar_csv(df)\n",
    "    print(\"se termino hasta: \",end)\n",
    "    start = end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
